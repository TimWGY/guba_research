{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label Chinese Stock Message Board Post Sentiment with Tensorflow - V1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dependencies**  \n",
    "numpy  \n",
    "jieba  \n",
    "gensim  \n",
    "tensorflow  \n",
    "matplotlib  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dependencies\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import jieba # for Chinese tokenization\n",
    "# gensim for loading pre-trained word vector\n",
    "from gensim.models import KeyedVectors\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# for decompression\n",
    "import bz2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Source of Pretrained Word Embedding**  \n",
    "https://github.com/Embedding/Chinese-Word-Vectors  \n",
    "Finance news domain-specific word embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # the bz2 file should be in the directory named embeddings\n",
    "# with open(\"embeddings/sgns.  .bigram\", 'wb') as new_file, open(\"embeddings/sgns.  .bigram.bz2\", 'rb') as file:\n",
    "#     decompressor = bz2.BZ2Decompressor()\n",
    "#     for data in iter(lambda : file.read(100 * 1024), b''):\n",
    "#         new_file.write(decompressor.decompress(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use gensim to load pre-trained embedding\n",
    "cn_model = KeyedVectors.load_word2vec_format('embeddings/sgns.financial.bigram-char', \n",
    "                                             binary=False, unicode_errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of word embedding is 300.\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = cn_model['慢'].shape[0]\n",
    "print('The length of word embedding is {}.'.format(embedding_dim))\n",
    "# cn_model['慢']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosine Similarity for Vector Space Models by Christian S. Perone\n",
    "# http://blog.christianperone.com/2013/09/machine-learning-cosine-similarity-for-vector-space-models-part-iii/\n",
    "# similarity of two words\n",
    "# cn_model.similarity('做空', '买')\n",
    "# np.dot(cn_model['反弹']/np.linalg.norm(cn_model['反弹']), \n",
    "# cn_model['涨']/np.linalg.norm(cn_model['涨']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('暴跌', 0.7939429879188538),\n",
       " ('重挫', 0.7413685321807861),\n",
       " ('下挫', 0.7085591554641724),\n",
       " ('下跌', 0.704810619354248),\n",
       " ('急跌', 0.7006838321685791),\n",
       " ('跳水', 0.658748984336853),\n",
       " ('挫', 0.6519291996955872),\n",
       " ('跌', 0.6498780250549316),\n",
       " ('暴涨', 0.6183834075927734),\n",
       " ('涨', 0.6176421642303467)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find most similar words to a keyword\n",
    "cn_model.most_similar(positive=['大跌'], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Among 反弹 大跌 下探 下挫 暴跌 亏损:\n",
      "亏损 is different from others.\n"
     ]
    }
   ],
   "source": [
    "# find word that doesn't belong to the category in a list\n",
    "test_words = '反弹 大跌 下探 下挫 暴跌 亏损'\n",
    "test_words_result = cn_model.doesnt_match(test_words.split())\n",
    "print('Among '+test_words+':\\n%s is different from others.' %test_words_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('强烈推荐', 0.419800341129303)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# concept pairing \n",
    "cn_model.most_similar(positive=['韭菜','买入'], negative=['庄家'], topn=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the hand-labeled stock message board data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_data_1 = pd.read_csv('sentiment_data_1.csv')\n",
    "sentiment_data_1.sentiment = sentiment_data_1.sentiment.apply(int)\n",
    "sentiment_data_1.sentiment = sentiment_data_1.sentiment.apply(lambda x: x if x>0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_data_1 = sentiment_data_1.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3727"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(sentiment_data_1['sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = sentiment_data_1[sentiment_data_1['sentiment']==1][:3720].append(sentiment_data_1[sentiment_data_1['sentiment']==0][:3720], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = dataset.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['title_body'] = dataset['post_title'] + dataset['post_body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7440\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts_orig = dataset['title_body'].to_list()\n",
    "train_target = dataset['sentiment'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing tensorflow APIs\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, GRU, Embedding, LSTM, Bidirectional\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.python.keras.optimizers import RMSprop\n",
    "from tensorflow.python.keras.optimizers import Adam\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Word cut and tokenization**  \n",
    "Remove punctuation, use jieba to cut words, turn resulting generator into list, index the words according to loaded pre-trained embeddings. In the end, each post will be turned into a list of indices, with each index corresponding to the word in the embedding matrix. The indice-list represenation of posts are stored in train_tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from /anaconda3/lib/python3.6/site-packages/jieba/dict.txt ...\n",
      "Loading model from cache /var/folders/lv/brdmj4x916vdfmgkd6k2s_3r0000gn/T/jieba.cache\n",
      "Loading model cost 0.9946861267089844 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "# Use jieba to parse and tokenize texts\n",
    "train_tokens = []\n",
    "for text in train_texts_orig:\n",
    "    # remove punctuations\n",
    "    text = re.sub(\"[\\s+\\.\\!\\/_,$%^*(+\\\"\\']+|[+——！，。？、~@#￥%……&*（）]+\", \"\",text)\n",
    "    # cut Chinese words with jieba\n",
    "    cut = jieba.cut(text)\n",
    "    # the outcome of jieba is a generator, the line below turns it into a list\n",
    "    cut_list = [ i for i in cut ]\n",
    "    for i, word in enumerate(cut_list):\n",
    "        try:\n",
    "            # turn word into index\n",
    "            cut_list[i] = cn_model.vocab[word].index\n",
    "        except KeyError:\n",
    "            # if word not in dictionary, use 0\n",
    "            cut_list[i] = 0\n",
    "    train_tokens.append(cut_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Normalize Post Index-list**  \n",
    "Posts vary in length. To save computational power and make sure all posts can be coded in the same shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the length of all posts\n",
    "num_tokens = [ len(tokens) for tokens in train_tokens ]\n",
    "num_tokens = np.array(num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(np.log(num_tokens), bins = 100)\n",
    "plt.xlim((0,10))\n",
    "plt.ylabel('number of tokens')\n",
    "plt.xlabel('length of tokens')\n",
    "plt.title('Distribution of tokens length')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assuming the length follows Guassian distribution...\n",
    "max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n",
    "max_tokens = int(max_tokens)\n",
    "max_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.964516129032258"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can cover 95% of the posts using 68, thus we can pad the short ones and truncate the long ones\n",
    "np.sum( num_tokens < max_tokens ) / len(num_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reverse-tokenization**  \n",
    "Define a function to map index to readable text, for debugging purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_tokens(tokens):\n",
    "    text = ''\n",
    "    for i in tokens:\n",
    "        if i != 0:\n",
    "            text = text + cn_model.index2word[i]\n",
    "        else:\n",
    "            text = text + ' '\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse = reverse_tokens(train_tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # However, punctuations are not retrieved.\n",
    "# reverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Comparing to original text\n",
    "# train_texts_orig[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prepare Embedding Matrix**  \n",
    "Keras needs a matrix with dimension $(numwords, embeddingdim)$\n",
    "To save computational power, use the embeddings of only first 100000 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = 100000\n",
    "# initialize embedding_matrix\n",
    "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "\n",
    "for i in range(num_words):\n",
    "    embedding_matrix[i,:] = cn_model[cn_model.index2word[i]]\n",
    "embedding_matrix = embedding_matrix.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check dimension and value of embedding are correct\n",
    "np.sum( cn_model[cn_model.index2word[333]] == embedding_matrix[333] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 300)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Padding and truncating**  \n",
    "Using 'pre' padding because some empirical studies suggest this practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad_sequences returns a numpy array\n",
    "train_pad = pad_sequences(train_tokens, maxlen=max_tokens,\n",
    "                            padding='pre', truncating='pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words not in the first 100000 are represented as 0\n",
    "train_pad[ train_pad>=num_words ] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,     0,     0,     0,     0,     0,     0,   303, 14618,\n",
       "        1413,   303, 14618,  1413,   303, 14618,  1413,   776, 19364,\n",
       "        9352, 82709,     0,   634,  1553, 24506,   104,     0, 22744,\n",
       "         125,   112,   303, 14618,  1413,   303, 14618,  1413,   303,\n",
       "       14618,  1413,   776, 19364,  9352, 82709,     0,   634,  1553,\n",
       "       24506,   104,     0, 22744,   125,   190,     0, 22744,    28,\n",
       "       34824,  8371,     0,  8562,   907,   448,   669,     0,   849,\n",
       "           0,  2129,   809,     0,  1467], dtype=int32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example\n",
    "train_pad[33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare target vector，first half are 1 (positive) second half are 0 (negative)\n",
    "train_target = np.array(train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train_target[len(train_target)//2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train test split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 90% as training，10% as testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_pad,\n",
    "                                                    train_target,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text:  上证指数3500左右震荡有望再 下走3450周一抄底周二周三   上证指数3500左右震荡有望再 下走3450周一抄底周二周三 多观望周四 短线获利落袋为安周四尾盘或周五注意风险\n",
      "class:  0\n"
     ]
    }
   ],
   "source": [
    "# checking training set \n",
    "print('text: ',reverse_tokens(X_train[35]).strip())\n",
    "print('class: ',y_train[35])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Start building model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding layer input: $$(batchsize, maxtokens)$$\n",
    "Embedding layer ouput: $$(batchsize, maxtokens, embeddingdim)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Embedding(num_words,\n",
    "                    embedding_dim,\n",
    "                    weights=[embedding_matrix],\n",
    "                    input_length=max_tokens,\n",
    "                    trainable=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Bidirectional(LSTM(units=64, return_sequences=True)))\n",
    "model.add(LSTM(units=16, return_sequences=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(GRU(units=32, return_sequences=True))\n",
    "model.add(GRU(units=16, return_sequences=True))\n",
    "model.add(GRU(units=4, return_sequences=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# optimizer = Adam(lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 68, 300)           30000000  \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 68, 128)           186880    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 68, 16)            9280      \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (None, 68, 32)            4704      \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 68, 16)            2352      \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (None, 4)                 252       \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 30,203,473\n",
      "Trainable params: 203,473\n",
      "Non-trainable params: 30,000,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# structure of model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weighting storage\n",
    "path_checkpoint = 'sentiment_checkpoint.keras'\n",
    "checkpoint = ModelCheckpoint(filepath=path_checkpoint, monitor='val_loss',\n",
    "                                      verbose=1, save_weights_only=True,\n",
    "                                      save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to open file (unable to open file: name = 'sentiment_checkpoint.keras', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)\n"
     ]
    }
   ],
   "source": [
    "# try_loading_pretrained_model\n",
    "try:\n",
    "    model.load_weights(path_checkpoint)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define early stoping if validation loss doesn't improve in 3 epochs\n",
    "earlystopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decreasing learning rate\n",
    "lr_reduction = ReduceLROnPlateau(monitor='val_loss',\n",
    "                                       factor=0.1, min_lr=1e-8, patience=0,\n",
    "                                       verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callback functions\n",
    "callbacks = [\n",
    "    earlystopping, \n",
    "    checkpoint,\n",
    "    lr_reduction\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5356 samples, validate on 596 samples\n",
      "Epoch 1/20\n",
      "5248/5356 [============================>.] - ETA: 1s - loss: 0.5462 - accuracy: 0.7222\n",
      "Epoch 00001: val_loss improved from inf to 0.40407, saving model to sentiment_checkpoint.keras\n",
      "5356/5356 [==============================] - 58s 11ms/sample - loss: 0.5427 - accuracy: 0.7248 - val_loss: 0.4041 - val_accuracy: 0.8289\n",
      "Epoch 2/20\n",
      "5248/5356 [============================>.] - ETA: 0s - loss: 0.3975 - accuracy: 0.8357\n",
      "Epoch 00002: val_loss improved from 0.40407 to 0.36268, saving model to sentiment_checkpoint.keras\n",
      "5356/5356 [==============================] - 36s 7ms/sample - loss: 0.3980 - accuracy: 0.8355 - val_loss: 0.3627 - val_accuracy: 0.8557\n",
      "Epoch 3/20\n",
      "5248/5356 [============================>.] - ETA: 0s - loss: 0.3481 - accuracy: 0.8639\n",
      "Epoch 00003: val_loss improved from 0.36268 to 0.34075, saving model to sentiment_checkpoint.keras\n",
      "5356/5356 [==============================] - 33s 6ms/sample - loss: 0.3475 - accuracy: 0.8639 - val_loss: 0.3407 - val_accuracy: 0.8624\n",
      "Epoch 4/20\n",
      "5248/5356 [============================>.] - ETA: 0s - loss: 0.3232 - accuracy: 0.8775\n",
      "Epoch 00004: val_loss improved from 0.34075 to 0.33646, saving model to sentiment_checkpoint.keras\n",
      "5356/5356 [==============================] - 31s 6ms/sample - loss: 0.3232 - accuracy: 0.8773 - val_loss: 0.3365 - val_accuracy: 0.8624\n",
      "Epoch 5/20\n",
      "5248/5356 [============================>.] - ETA: 0s - loss: 0.2830 - accuracy: 0.8971\n",
      "Epoch 00005: val_loss improved from 0.33646 to 0.32792, saving model to sentiment_checkpoint.keras\n",
      "5356/5356 [==============================] - 34s 6ms/sample - loss: 0.2844 - accuracy: 0.8966 - val_loss: 0.3279 - val_accuracy: 0.8641\n",
      "Epoch 6/20\n",
      "5248/5356 [============================>.] - ETA: 0s - loss: 0.2691 - accuracy: 0.9057\n",
      "Epoch 00006: val_loss did not improve from 0.32792\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "5356/5356 [==============================] - 32s 6ms/sample - loss: 0.2706 - accuracy: 0.9055 - val_loss: 0.3570 - val_accuracy: 0.8389\n",
      "Epoch 7/20\n",
      "5248/5356 [============================>.] - ETA: 0s - loss: 0.2382 - accuracy: 0.9224\n",
      "Epoch 00007: val_loss did not improve from 0.32792\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "5356/5356 [==============================] - 30s 6ms/sample - loss: 0.2380 - accuracy: 0.9225 - val_loss: 0.3290 - val_accuracy: 0.8742\n",
      "Epoch 8/20\n",
      "5248/5356 [============================>.] - ETA: 0s - loss: 0.2199 - accuracy: 0.9301\n",
      "Epoch 00008: val_loss improved from 0.32792 to 0.32444, saving model to sentiment_checkpoint.keras\n",
      "5356/5356 [==============================] - 31s 6ms/sample - loss: 0.2196 - accuracy: 0.9305 - val_loss: 0.3244 - val_accuracy: 0.8775\n",
      "Epoch 9/20\n",
      "5248/5356 [============================>.] - ETA: 0s - loss: 0.2188 - accuracy: 0.9314\n",
      "Epoch 00009: val_loss did not improve from 0.32444\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "5356/5356 [==============================] - 36s 7ms/sample - loss: 0.2181 - accuracy: 0.9313 - val_loss: 0.3252 - val_accuracy: 0.8775\n",
      "Epoch 10/20\n",
      "5248/5356 [============================>.] - ETA: 0s - loss: 0.2165 - accuracy: 0.9312\n",
      "Epoch 00010: val_loss did not improve from 0.32444\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "5356/5356 [==============================] - 33s 6ms/sample - loss: 0.2171 - accuracy: 0.9311 - val_loss: 0.3252 - val_accuracy: 0.8775\n",
      "Epoch 11/20\n",
      "5248/5356 [============================>.] - ETA: 0s - loss: 0.2150 - accuracy: 0.9322\n",
      "Epoch 00011: val_loss did not improve from 0.32444\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "5356/5356 [==============================] - 31s 6ms/sample - loss: 0.2170 - accuracy: 0.9315 - val_loss: 0.3252 - val_accuracy: 0.8775\n",
      "Epoch 12/20\n",
      "5248/5356 [============================>.] - ETA: 0s - loss: 0.2158 - accuracy: 0.9318\n",
      "Epoch 00012: val_loss did not improve from 0.32444\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 1e-08.\n",
      "5356/5356 [==============================] - 27s 5ms/sample - loss: 0.2170 - accuracy: 0.9315 - val_loss: 0.3252 - val_accuracy: 0.8775\n",
      "Epoch 13/20\n",
      "5248/5356 [============================>.] - ETA: 0s - loss: 0.2163 - accuracy: 0.9316\n",
      "Epoch 00013: val_loss did not improve from 0.32444\n",
      "5356/5356 [==============================] - 28s 5ms/sample - loss: 0.2170 - accuracy: 0.9315 - val_loss: 0.3252 - val_accuracy: 0.8775\n",
      "Epoch 00013: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x180f99fd0>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# start fitting\n",
    "model.fit(X_train, y_train,\n",
    "          validation_split=0.1, \n",
    "          epochs=20,\n",
    "          batch_size=128,\n",
    "          callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate with test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1488/1488 [==============================] - 3s 2ms/sample - loss: 0.3508 - accuracy: 0.8542\n",
      "Accuracy:85.42%\n"
     ]
    }
   ],
   "source": [
    "result = model.evaluate(X_test, y_test)\n",
    "print('Accuracy:{0:.2%}'.format(result[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(text):\n",
    "    print(text)\n",
    "    # remove punctuations\n",
    "    text = re.sub(\"[\\s+\\.\\!\\/_,$%^*(+\\\"\\']+|[+——！，。？、~@#￥%……&*（）]+\", \"\",text)\n",
    "    # cut words\n",
    "    cut = jieba.cut(text)\n",
    "    cut_list = [ i for i in cut ]\n",
    "    # tokenize\n",
    "    for i, word in enumerate(cut_list):\n",
    "        try:\n",
    "            cut_list[i] = cn_model.vocab[word].index\n",
    "            if cut_list[i] >= 50000:\n",
    "                cut_list[i] = 0\n",
    "        except KeyError:\n",
    "            cut_list[i] = 0\n",
    "    # padding\n",
    "    tokens_pad = pad_sequences([cut_list], maxlen=max_tokens,\n",
    "                           padding='pre', truncating='pre')\n",
    "    # prediction\n",
    "    result = model.predict(x=tokens_pad)\n",
    "    coef = result[0][0]\n",
    "    if coef >= 0.5:\n",
    "        print('postive','output=%.2f'%coef)\n",
    "    else:\n",
    "        print('negative','output=%.2f'%coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "无量上涨，冲高回落\n",
      "postive output=0.85\n",
      "大盘已经跌到底了，散户们冲啊抄底\n",
      "postive output=0.89\n",
      "为国护盘，坚定持有\n",
      "postive output=0.91\n",
      "吧里的多头去哪里了呢\n",
      "negative output=0.36\n",
      "这轮反弹即将结束，建议大家逢高撤出\n",
      "postive output=0.91\n",
      "明显是庄家诱多，傻散才会抄底呢[傲]\n",
      "postive output=0.93\n",
      "明天开盘大涨，打爆这些空头\n",
      "negative output=0.37\n"
     ]
    }
   ],
   "source": [
    "test_list = [\n",
    "    '无量上涨，冲高回落',\n",
    "    '大盘已经跌到底了，散户们冲啊抄底',\n",
    "    '为国护盘，坚定持有',\n",
    "    '吧里的多头去哪里了呢',\n",
    "    '这轮反弹即将结束，建议大家逢高撤出',\n",
    "    '明显是庄家诱多，傻散才会抄底呢[傲]',\n",
    "    '明天开盘大涨，打爆这些空头'\n",
    "]\n",
    "for text in test_list:\n",
    "    predict_sentiment(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Misclassified posts**<br>\n",
    "The concept of '反弹' is not properly learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred = y_pred.T[0]\n",
    "y_pred = [1 if p>= 0.5 else 0 for p in y_pred]\n",
    "y_pred = np.array(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_actual = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "misclassified = np.where( y_pred != y_actual )[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "217/1488\n"
     ]
    }
   ],
   "source": [
    "print(len(misclassified),end='/')\n",
    "print(len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "久违的逼空光头大阳盼来了久违的逼空光头大阳盼来了\n",
      "预测的分类 1\n",
      "实际的分类 0\n"
     ]
    }
   ],
   "source": [
    "# example of misclassified\n",
    "idx = misclassified[2]\n",
    "print(reverse_tokens(X_test[idx]).strip())\n",
    "print('预测的分类', y_pred[idx])\n",
    "print('实际的分类', y_actual[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       市场资金正在从高价股转向低价股关注600512腾达建设浙江板块工程建设板块市场资金正在从高价股转向低价股关注600512腾达建设浙江板块工程建设板块双料低价冠军后续订单 定向增发停牌重组可期股价528元\n",
      "预测的分类 0\n",
      "实际的分类 0\n"
     ]
    }
   ],
   "source": [
    "# example of correcetly classified\n",
    "idx=1\n",
    "print(reverse_tokens(X_test[idx]))\n",
    "print('预测的分类', y_pred[idx])\n",
    "print('实际的分类', y_actual[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# References\n",
    "# https://github.com/aespresso/chinese_sentiment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
